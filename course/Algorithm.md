# 算法导论

## 背景

有人云，程序=算法+数据结构。先不论这个有没有卵用，但还是说明了些一个程序必要的部分。

数据结构是你怎么存储你的数据，算法是你如何处理你的数据，恩恩，很有道理。

本想考试前就边复习边写，但是盯着电脑效率就低下来了，于是咕咕；考完考跪后，虽然再写这个犹如自揭伤疤，但是还是忍痛写了下来。

## 目录

<p id="menu"></p>

* [时间复杂度的概念](#timecost)

	如何知道你的程序运行会花多长时间？

* [排序算法](#sort)
	* [n^2复杂度排序](#n2sort)
	* [nlogn复杂度排序](#nlognsort)
	* [线性复杂度排序](#nsort)
	* [排序算法变种](#aroundsort)

	如何对无序数进行排序？可以基于互相比较的排序，也可以进行哈希散列后排序。基于比较的排序最快的复杂度为O(nlogn)，而基于散列的期望复杂度可以到达O(n)复杂度。

	变种的话，因为有的时候具体问题不一定需要完全有序就可解答，或者在排序的时候可以顺带求一些参量，所以出现了这一部分。

* [树](#tree)
	* 二叉搜索树
	* 红黑树
	* 红黑树扩张
	
	红黑树上红黑果，红黑树下你和我。
	
	考前大雪，压垮了学校一堆红黑树。
	
* [算法设计与分析](#algdesign)
	* 动态规划-备忘与自底向上
	* 贪心
	* 摊还分析
	
## 若干个算法和数据结构

<p id="timecost"></p>

### 1. 时间复杂度的概念

太复杂的证明我个人其实认为挺没必要的，因为复杂的真的就是纯玩数学而已。虽然我不否定数学的重要性，不过把过于复杂的数学揉到计算机里面其实挺没意思的；就相当于做随机过程算Markov链却要你手解7阶方阵的特征根一样恶心。当然，考试就在这里考跪了，要求递推式的精确系数，还要平移函数...完全的数学问题，只可惜太久不碰，怎么可能考场那一会就能解得出来。

时间复杂度通俗来讲，就是按照现在电脑指令体系结构的简单抽象，需要执行多少条指令就是时间复杂度，是数据量n的函数。比如循环遍历数组，就是O(n)。

g(n)=O(f(n)) 存在c，n->∞ g(n)<cf(n)
g(n)=o(f(n)) n->∞, g(n)/f(n) -> 0

g(n)=Ω(f(n)) 存在c，n->∞ g(n)>cf(n)
g(n)=w(f(n)) n->∞, g(n)/f(n) -> ∞

g(n)=θ(f(n)) 存在c1,c2 n->∞ c1*f(n)<g(n)<c2*f(n)

说白了，就是你这段程序随着数据量的变化，会执行指令的条数的函数的上下界。

有的时候，我们的程序有递归写法，这个时候就要依赖“主方法”。如同教科书般的问题解决方案：

如果 T(n)=a*T(n/b)+f(n)：
1. n^(log(a,b))<f(n)  ==> T(n)=θ(f(n))
2. n^(log(a,b))==f(n) ==> T(n)=θ(f(n)*log(2,n))
3. n^(log(a,b))>f(n)  ==> T(n)=θ(n^(log(a,b)))

递归有其他处理方法，比如一个猜解+数学归纳，靠运气和直觉；一个递归树求和，等比数列计算，太偏数学。不提了。

[回到索引](#menu)

<p id="sort"></p>

### 2. 各种排序

<p id="n2sort"></p>

**稳定排序：元素值相同的元素，在排序完以后，还保持着和排序前一样的先后次序。**

e.g. 对向量长度进行排序，(0,1) (2,2) (1,0)

稳定排序要求答案中(0,1)在(1,0)前面。

* 冒泡与选择

	O(n^2)复杂度，简单而慢。冒泡是不停交换相邻两个数的位置，一趟结束后最右是最大/小的数；选择是每次单趟结束，找出最大/小的，再进行交换，放到一边，然后对剩下的部分重复操作。

	经典代码结构：

	```C++
	void bubble(int *array,int n) {
		int i,j,temp;
		for(i=0;i<n-1;++i) { //排序n-1趟
			for(j=0;j<n-i-1;++j) {
				if(array[j]<array[j+1]) {
					temp=array[j];
					array[j]=array[j+1];
					array[j+1]=temp;
				}
			}
		}
	}
	//双重循环内嵌交换，为冒泡

	void select(int *array,int n) {
		int i,j,k,max,temp;
		for(i=0;i<n-1;++i) { //排序n-1趟
			k=i;max=array[k];
			for(j=i;j<n;++j) {
				if(array[j]<max) {
					max=array[j];
					k=j;
				}
			}
			if(k!=i) {
				temp=array[i];
				array[i]=max;
				array[k]=temp;
			}
		}
	}
	//双重循环，内层先赋值max再循环再交换的，为选择

	```

* 插入排序

	想法与上面似乎有些不同，如同操场站队。这个排序是一开始一个数，然后不停往里面插数。插到一个地方后，后面所有的数都要往后挪一个格子。显然，用链表理论来讲肯定会快。

	[回到索引](#menu)

---

<p id="nlognsort"></p>

* 归并排序

	想法是，假设我们已经有两个升序序列，我们可以在O(n)的时间内将其合并成一个总的升序序列。(不停检查序列1,2的开头，并且取出较小的放到新序列的尾部)。这明显是一个递归下降的问题，即下降到小常数，比如只剩下一个数或者两个数或者没有数，可以直接返回(注意考虑整数除二上下取舍的问题)。即：

	T(n)=2*T(n/2)+2n 根据主方法，轻易算得O(nlogn)复杂度

	于是，伪代码如此：

	```python
	def combine(A,begin,mid,end):
		B=[]
		swap(A,B)
		i=begin,j=mid;
		while i<mid and j<end：
			if B[i]<=B[j]: A.append(B[i]);i+=1;
			else: A.append(B[j]);j+=1;
		while i<mid:
			A.append(B[i]);i+=1;
		while j<end:
			A.append(B[j]);j+=1;

	def merge(A,i,j):
		if i==j-1 or i==j: return;
		else:
			merge(A,i,(i+j)/2)
			merge(A,(i+j)/2+1,j)
			combine(A,i,(i+j)/2+1,j)
		return;
	```
	显然，归并的速度很快，但是会额外耗去大量的空间，不是原地排序。

	[回到索引](#menu)

* 快速排序

	感觉很多地方写的有点妖魔化了。本质很简单，随便瞎选个数，比这个数大的放右边，小的放左边。遍历一遍后( O(n) )，这个数位置就确定了，然后对两侧进行递归。

	T(n)=2*T(n/2)+n ，答案依然为O(nlogn)复杂度。当然，因为你不可能一直是均分，所以最差复杂度是Ω(n^2)。简单的例子就是，对逆序的序列进行顺序排序。

	```python
	def partition(A,i,j):
		pivot=A[j-1]
		a=i-1
		b=i
		for b in range(i,j-1):
			if A[b]<pivot:
				a+=1
				swap(A,b,a)	
		swap(A,a+1,j-1)
		return a+1
	def quicksort(A,i,j):
		if i==j or i==j-1: return;
		else:
			a=partition(A,i,j)
			quicksort(A,i,a)
			quicksort(A,a+1,j)
	```

	partition很简单，选择数组最后一个元素作为分隔符。遍历数组分为三段：小段，大段和未遍历到的段。如果未遍历的部分比分隔符大，大段增长；如果小，则把这个和大段的第一个元素交换，相当于小段增长。遍历完后，要把分隔符放进去，和大段第一个元素交换即可。

	[回到索引](#menu)

* 堆排序

	可能很多人都被各种书籍关于快速排序、堆排序的各种算法介绍绕晕了。这些算法的本质很简单，只不过不知道为什么那些书就不肯把基本的核心算法讲清楚，反而上来就伪代码解释来解释去。

	堆排序的核心是二叉大根/小根树，即在这棵二叉树里面，所有父节点比子节点都要大。

	假设我们已经有了这棵树(设为大根树)，现在要进行排序。最大的当然是根节点啦——根>左右子节点>左右子节点的子节点>...>叶节点。

	现在，我们取出了最大的结点，树缺了一块。很简单，我们可以把某个没有子节点的叶节点补上去嘛。但是会出现问题：叶节点八成会破坏大根的性质。这个时候，我们需要一个维护的过程：

	O. 无子节点 -> 结束

	I. 新的父节点比两个子节点都大 -> 不破坏性质，结束

	II. 新的父节点要小 -> 与最大子节点交换位置，然后对子节点再次判断。

	维护完了后，根节点又是最大的数了，再次取出->维护，重复进行即可。

	现在最大的问题是：一开始的数据是无序的，怎么让其变成大根或者小根树？

	很简单啊，先从叶结点开始往根节点的方向，每个节点维护一次啊。这不就保证了，任何结点在维护之前，其两个子节点已经被维护过了吗？

	维护复杂度：T(n)=T(n/2)+c --> T(n)=O(logn)
	
	建堆复杂度：T(n)=n*维护=O(nlogn)

	排序复杂度：T(n)=O(logn+log(n-1)+...+log(1))=O(log(n!))=O(nlogn)

	总复杂度： O(nlogn)

	实际上，我们一般用数组来实现这棵树。数组下标从0开始的话，i为根，则2i+1为左子，2i+2为右子；(i-1)/2为父。

	[回到索引](#menu)

---

<p id="nsort"></p>

* 线性时间复杂度排序

	基于元素之间两两比较的最快也要nlogn，但如果不基于比较，而是知道数据范围的话，可以在线性的期望时间内完成：

	第一种，计数排序。

	p=100 -> array[100]++;

	就这么简单。考虑到要稳定排序，需要算前缀和，然后从原待排序序列从末尾往前遍历，根据前缀和数组找到其位置。

	e.g.: 对 3.1 5.1 5.2 3.2 的整数部分进行排序。小数部分只区分先后顺序。

	第一步，计算数组(1下标开始)后：[0,0,2,0,2]

	第二步：算前缀和：[0,0,2,2,4]

	第三步，对3.1 5.1 5.2 3.2从后往前，第一个看到3，然后查前缀和第三项，是2，那么答案的ans[2]=3.2，然后2自减。
	
	临时结果：ans:[0 3.2 0 0] 前缀:[0,0,1,2,4]

	然后看到5.2，查5，是4，则ans[4]=5.2 4自减为3.

	临时结果：ans:[0 3.2 0 5.2] 前缀:[0,0,1,2,3]

	重复。时间复杂度：O(n)

	第二种，基数排序。

	先对个位排序，再对十位排序，然后百位...用稳定排序...

	没了！时间复杂度：O(d*f(n)) d为位数，f(n)为稳定排序时间

	第三种，桶排序。

	就是一个分类的过程。比如大于1000一个桶，其他里面大于100一个桶，小于等于100一个桶。桶里面需要用其他排序来维持关系。

	桶选的好的话，效率会非常高。最快的话时间复杂度为O(n+m) m为桶的个数。

	[回到索引](#menu)

---

<p id="aroundsort"></p>
	
* 排序算法变种

	说是变种，其实就是基于某个已有的排序算法的思想，来获得我们需要的结果而已。
	
	第一，逆序对。也就是一个数组中有多少个i<j却a[i]>a[j]的数对的个数。很简单，用归并排序，每次combine的时候发现a[i]>a[j]就对count自增。
	
	第二，期望时间为O(n)的选择第k大的数。直接运行排的 partition ，然后看返回的位置决定往左找还是往右找还是已经找到了。
	
	[回到索引](#menu)
	
---

---

<p id="tree"></p>

### 3. 树

* 二叉搜索树

	太简单就不说了，数据结构应该学的。插入、查找都很简单，时间复杂度均为O(logn)。删除的话也是O(logn)，删除操作比较不直观。如果结点最多只有一个子节点，那么直接移花接木把子节点(或者nil)替换上来即可。如果有两个子节点，那么就找后继。后继可以在删除节点的右孩子不停往左孩子找直到某结点没有左孩子，这就是后继。后继替换原被删结点，后继右子树替换后继。
	
* 红黑树

	满足五个性质：
		* 每个点非黑即红
		* 根为黑
		* 根到每个叶节点经过的黑结点个数相同
		* 若有红结点，其父与子均必须为黑结点
		* 所有叶节点全部为黑节点
		
	查找不变。插入、删除的基本操作也不变，不过因为会破坏黑结点个数（黑高），所以需要左旋右旋、分情况等诸多琐事。网上有一堆教程。
	
	红黑树的优势在于，因为黑高一定而红结点不相邻，最长路径最多也只是最短路径的两倍不到。这保证了查找的优势。同时，红黑树维护的期望代价并不高，没有增加复杂度量级。
	
* 红黑树扩张

	就是每个节点多个size域=lchild.size+rchild.size+1 ，用来从红黑树里面选出第k大的数，或者判断某节点在中缀表达式里面的位置。
	
	另一个扩展是区间树，多个维护区间左右端点的区别。
	
	[回到索引](#menu)
	
---

---

<p id="algdesign"></p>
	
### 4. 算法设计与分析

* 动态规划-备忘与自底向上

《算法导论》是用一个钢条切割引入这个问题的。题目很简单，不同长度的钢条有不同的价格，假设切割不需要花钱，请问你有一段长为L的钢条，怎么切割能使获利最大。比如，切成w1,w2,w3...长度，对应单价为p1,p2,p3...，则总获利为w1p1+w2p2+w3p3...。

如果暴力枚举所有的解肯定太慢，这个时候有这样一个思路：

第一种情况：钢条不需要切割，第二种情况：钢条第一次在x处切割。

所以， f(L)=max( p(L), f(x)+f(L-x) ) x在1到L-1之间遍历。当然，可以图省事只遍历一半，另一半是对称的。

这就是动态规划的核心思想。

然后是叫备忘机制。大家都知道斐波那契的递归写法：

```C++
int fib(int n) {
	if(n==1||n==2) return 1;
	else return fib(n-1)+fib(n-2);
}
```

然而这么写速度会慢到爆表。但我们可以对中间值进行缓存：

```C++
int mem[10001]={-9999,1,1};
int fib(int n) {
	if(mem[n]!=0) return mem[n];
	else {
		mem[n]=fib(n-1)+fib(n-2);
		return mem[n];
	}
}
```

每一个值只需要计算一次。

这就是所谓的备忘机制。

最后谈一谈自底向上。刚才斐波那契的程序在n比较大的时候还是会爆表，这个时候可以换成递推的写法：

```C++
int mem[10001]={-9999,1,1};
int fib(int n) {
	for(int i=3;i<=n;++i) {
		if(mem[i]!=0) continue;
		mem[i]=mem[i-1]+mem[i-2];
	}
	return mem[n];
}
```

如果只求解一次的话，显然是递推的手法效率最高。

比如像刚才那个钢条切割问题，递推的思路就是：

首先，f(1)肯定等于p(1)啦，那f(2)是max(p(2),2*f(1))，f(3)是max(p(3),f(1)+f(2))，f(4)是max(p(4),f(1)+f(3),2*f(2))，依次类推。

不过有一点，**动态规划要求最优子结构**。什么意思呢？先举个例子：有一个正方形，要求点之间的**最长简单路径**。顺时针排序1234，1到2、2到3最长简单路径当然是3，但是1到3的最长简单路径并不是6，而是2.

也就是说，最优解应该是由另外若干个局部最优解组成的。满足这种性质的才可以用动态规划。

**后面的内容之后再更新...**

* 贪心

书上用活动安排问题来引出的。