# 算法导论

## 背景

有人云，程序=算法+数据结构。先不论这个有没有卵用，但还是说明了些一个程序必要的部分。

数据结构是你怎么存储你的数据，算法是你如何处理你的数据，恩恩，很有道理。

本想考试前就边复习边写，但是盯着电脑效率就低下来了，于是咕咕；考完考跪后，虽然再写这个犹如自揭伤疤，但是还是忍痛写了下来。

## 目录

<p id="menu"></p>

* [时间复杂度的概念](#timecost)

	如何知道你的程序运行会花多长时间？

* [排序算法](#sort)
	* [n^2复杂度排序](#n2sort)
	* [nlogn复杂度排序](#nlognsort)
	* [线性复杂度排序](#nsort)
	* [排序算法变种](#aroundsort)

	如何对无序数进行排序？可以基于互相比较的排序，也可以进行哈希散列后排序。基于比较的排序最快的复杂度为O(nlogn)，而基于散列的期望复杂度可以到达O(n)复杂度。

	变种的话，因为有的时候具体问题不一定需要完全有序就可解答，或者在排序的时候可以顺带求一些参量，所以出现了这一部分。

* [树](#tree)
	* 二叉搜索树
	* 红黑树
	* 红黑树扩张
	
	红黑树上红黑果，红黑树下你和我。
	
	考前大雪，压垮了学校一堆红黑树。
	
* [算法设计与分析](#algdesign)
	* 动态规划-备忘与自底向上
	* 贪心
	* 摊还分析
	* 动态表
	
* [杂项](#other)

* [总结](#end)
	
## 若干个算法和数据结构

<p id="timecost"></p>

### 1. 时间复杂度的概念

太复杂的证明我个人其实认为挺没必要的，因为复杂的真的就是纯玩数学而已。虽然我不否定数学的重要性，不过把过于复杂的数学揉到计算机里面其实挺没意思的；就相当于做随机过程算Markov链却要你手解7阶方阵的特征根一样恶心。当然，考试就在这里考跪了，要求递推式的精确系数，还要平移函数...完全的数学问题，只可惜太久不碰，怎么可能考场那一会就能解得出来。

时间复杂度通俗来讲，就是按照现在电脑指令体系结构的简单抽象，需要执行多少条指令就是时间复杂度，是数据量n的函数。比如循环遍历数组，就是O(n)。

g(n)=O(f(n)) 存在c，n->∞ g(n)<cf(n)
g(n)=o(f(n)) n->∞, g(n)/f(n) -> 0

g(n)=Ω(f(n)) 存在c，n->∞ g(n)>cf(n)
g(n)=w(f(n)) n->∞, g(n)/f(n) -> ∞

g(n)=θ(f(n)) 存在c1,c2 n->∞ c1f(n)<g(n)<c2f(n)

说白了，就是你这段程序随着数据量的变化，会执行指令的条数的函数的上下界。

有的时候，我们的程序有递归写法，这个时候就要依赖“主方法”。如同教科书般的问题解决方案：

如果 T(n)=a*T(n/b)+f(n)：
1. n^(log(a,b))<f(n)  ==> T(n)=θ(f(n))
2. n^(log(a,b))==f(n) ==> T(n)=θ(f(n)*log(2,n))
3. n^(log(a,b))>f(n)  ==> T(n)=θ(n^(log(a,b)))

递归有其他处理方法，比如一个猜解+数学归纳，靠运气和直觉；一个递归树求和，等比数列计算，太偏数学。不提了。

[回到索引](#menu)

<p id="sort"></p>

### 2. 各种排序

<p id="n2sort"></p>

**稳定排序：元素值相同的元素，在排序完以后，还保持着和排序前一样的先后次序。**

e.g. 对向量长度进行排序，(0,1) (2,2) (1,0)

稳定排序要求答案中(0,1)在(1,0)前面。

* 冒泡与选择

	O(n^2)复杂度，简单而慢。冒泡是不停交换相邻两个数的位置，一趟结束后最右是最大/小的数；选择是每次单趟结束，找出最大/小的，再进行交换，放到一边，然后对剩下的部分重复操作。

	经典代码结构：

	```C++
	void bubble(int *array,int n) {
		int i,j,temp;
		for(i=0;i<n-1;++i) { //排序n-1趟
			for(j=0;j<n-i-1;++j) {
				if(array[j]<array[j+1]) {
					temp=array[j];
					array[j]=array[j+1];
					array[j+1]=temp;
				}
			}
		}
	}
	//双重循环内嵌交换，为冒泡

	void select(int *array,int n) {
		int i,j,k,max,temp;
		for(i=0;i<n-1;++i) { //排序n-1趟
			k=i;max=array[k];
			for(j=i;j<n;++j) {
				if(array[j]<max) {
					max=array[j];
					k=j;
				}
			}
			if(k!=i) {
				temp=array[i];
				array[i]=max;
				array[k]=temp;
			}
		}
	}
	//双重循环，内层先赋值max再循环再交换的，为选择

	```

* 插入排序

	想法与上面似乎有些不同，如同操场站队。这个排序是一开始一个数，然后不停往里面插数。插到一个地方后，后面所有的数都要往后挪一个格子。显然，用链表理论来讲肯定会快。

	[回到索引](#menu)

---

<p id="nlognsort"></p>

* 归并排序

	想法是，假设我们已经有两个升序序列，我们可以在O(n)的时间内将其合并成一个总的升序序列。(不停检查序列1,2的开头，并且取出较小的放到新序列的尾部)。这明显是一个递归下降的问题，即下降到小常数，比如只剩下一个数或者两个数或者没有数，可以直接返回(注意考虑整数除二上下取舍的问题)。即：

	T(n)=2*T(n/2)+2n 根据主方法，轻易算得O(nlogn)复杂度

	于是，伪代码如此：

	```python
	def combine(A,begin,mid,end):
		B=[]
		swap(A,B)
		i=begin,j=mid;
		while i<mid and j<end:
			if B[i]<=B[j]: A.append(B[i]);i+=1;
			else: A.append(B[j]);j+=1;
		while i<mid:
			A.append(B[i]);i+=1;
		while j<end:
			A.append(B[j]);j+=1;

	def merge(A,i,j):
		if i==j-1 or i==j: return;
		else:
			merge(A,i,(i+j)/2)
			merge(A,(i+j)/2+1,j)
			combine(A,i,(i+j)/2+1,j)
		return;
	```
	显然，归并的速度很快，但是会额外耗去大量的空间，不是原地排序。

	[回到索引](#menu)

* 快速排序

	感觉很多地方写的有点妖魔化了。本质很简单，随便瞎选个数，比这个数大的放右边，小的放左边。遍历一遍后( O(n) )，这个数位置就确定了，然后对两侧进行递归。

	T(n)=2*T(n/2)+n ，答案依然为O(nlogn)复杂度。当然，因为你不可能一直是均分，所以最差复杂度是Ω(n^2)。简单的例子就是，对逆序的序列进行顺序排序。

	```python
	def partition(A,i,j):
		pivot=A[j-1]
		a=i-1
		b=i
		for b in range(i,j-1):
			if A[b]<pivot:
				a+=1
				swap(A,b,a)	
		swap(A,a+1,j-1)
		return a+1
	def quicksort(A,i,j):
		if i==j or i==j-1: return;
		else:
			a=partition(A,i,j)
			quicksort(A,i,a)
			quicksort(A,a+1,j)
	```

	partition很简单，选择数组最后一个元素作为分隔符。遍历数组分为三段：小段，大段和未遍历到的段。如果未遍历的部分比分隔符大，大段增长；如果小，则把这个和大段的第一个元素交换，相当于小段增长。遍历完后，要把分隔符放进去，和大段第一个元素交换即可。

	[回到索引](#menu)

* 堆排序

	可能很多人都被各种书籍关于快速排序、堆排序的各种算法介绍绕晕了。这些算法的本质很简单，只不过不知道为什么那些书就不肯把基本的核心算法讲清楚，反而上来就伪代码解释来解释去。

	堆排序的核心是二叉大根/小根树，即在这棵二叉树里面，所有父节点比子节点都要大。

	假设我们已经有了这棵树(设为大根树)，现在要进行排序。最大的当然是根节点啦——根>左右子节点>左右子节点的子节点>...>叶节点。

	现在，我们取出了最大的结点，树缺了一块。很简单，我们可以把某个没有子节点的叶节点补上去嘛。但是会出现问题：叶节点八成会破坏大根的性质。这个时候，我们需要一个维护的过程：

	O. 无子节点 -> 结束

	I. 新的父节点比两个子节点都大 -> 不破坏性质，结束

	II. 新的父节点要小 -> 与最大子节点交换位置，然后对子节点再次判断。

	维护完了后，根节点又是最大的数了，再次取出->维护，重复进行即可。

	现在最大的问题是：一开始的数据是无序的，怎么让其变成大根或者小根树？

	很简单啊，先从叶结点开始往根节点的方向，每个节点维护一次啊。这不就保证了，任何结点在维护之前，其两个子节点已经被维护过了吗？

	维护复杂度：T(n)=T(n/2)+c --> T(n)=O(logn)
	
	建堆复杂度：T(n)=n*维护=O(nlogn)

	排序复杂度：T(n)=O(logn+log(n-1)+...+log(1))=O(log(n!))=O(nlogn)

	总复杂度： O(nlogn)

	实际上，我们一般用数组来实现这棵树。数组下标从0开始的话，i为根，则2i+1为左子，2i+2为右子；(i-1)/2为父。

	[回到索引](#menu)

---

<p id="nsort"></p>

* 线性时间复杂度排序

	基于元素之间两两比较的最快也要nlogn，但如果不基于比较，而是知道数据范围的话，可以在线性的期望时间内完成：

	第一种，计数排序。

	p=100 -> array[100]++;

	就这么简单。考虑到要稳定排序，需要算前缀和，然后从原待排序序列从末尾往前遍历，根据前缀和数组找到其位置。

	e.g.: 对 3.1 5.1 5.2 3.2 的整数部分进行排序。小数部分只区分先后顺序。

	第一步，计算数组(1下标开始)后：[0,0,2,0,2]

	第二步：算前缀和：[0,0,2,2,4]

	第三步，对3.1 5.1 5.2 3.2从后往前，第一个看到3，然后查前缀和第三项，是2，那么答案的ans[2]=3.2，然后2自减。
	
	临时结果：ans:[0 3.2 0 0] 前缀:[0,0,1,2,4]

	然后看到5.2，查5，是4，则ans[4]=5.2 4自减为3.

	临时结果：ans:[0 3.2 0 5.2] 前缀:[0,0,1,2,3]

	重复。时间复杂度：O(n)

	第二种，基数排序。

	先对个位排序，再对十位排序，然后百位...用稳定排序...

	没了！时间复杂度：O(d*f(n)) d为位数，f(n)为稳定排序时间

	第三种，桶排序。

	就是一个分类的过程。比如大于1000一个桶，其他里面大于100一个桶，小于等于100一个桶。桶里面需要用其他排序来维持关系。

	桶选的好的话，效率会非常高。最快的话时间复杂度为O(n+m) m为桶的个数。

	[回到索引](#menu)

---

<p id="aroundsort"></p>
	
* 排序算法变种

	说是变种，其实就是基于某个已有的排序算法的思想，来获得我们需要的结果而已。
	
	第一，逆序对。也就是一个数组中有多少个i<j却a[i]>a[j]的数对的个数。很简单，用归并排序，每次combine的时候发现a[i]>a[j]就对count自增。
	
	第二，期望时间为O(n)的选择第k大的数。直接运行排的 partition ，然后看返回的位置决定往左找还是往右找还是已经找到了。
	
	[回到索引](#menu)
	
---

---

<p id="tree"></p>

### 3. 树

* 二叉搜索树

	太简单就不说了，数据结构应该学的。插入、查找都很简单，时间复杂度均为O(logn)。删除的话也是O(logn)，删除操作比较不直观。如果结点最多只有一个子节点，那么直接移花接木把子节点(或者nil)替换上来即可。如果有两个子节点，那么就找后继。后继可以在删除节点的右孩子不停往左孩子找直到某结点没有左孩子，这就是后继。后继替换原被删结点，后继右子树替换后继。
	
* 红黑树

	满足五个性质：
		* 每个点非黑即红
		* 根为黑
		* 根到每个叶节点经过的黑结点个数相同
		* 若有红结点，其父与子均必须为黑结点
		* 所有叶节点全部为黑节点
		
	查找不变。插入、删除的基本操作也不变，不过因为会破坏黑结点个数（黑高），所以需要左旋右旋、分情况等诸多琐事。网上有一堆教程。
	
	红黑树的优势在于，因为黑高一定而红结点不相邻，最长路径最多也只是最短路径的两倍不到。这保证了查找的优势。同时，红黑树维护的期望代价并不高，没有增加复杂度量级。
	
* 红黑树扩张

	就是每个节点多个size域=lchild.size+rchild.size+1 ，用来从红黑树里面选出第k大的数，或者判断某节点在中缀表达式里面的位置。
	
	另一个扩展是区间树，多个维护区间左右端点的区别。
	
	[回到索引](#menu)
	
---

---

<p id="algdesign"></p>
	
### 4. 算法设计与分析

* 动态规划-备忘与自底向上

	《算法导论》是用一个钢条切割引入这个问题的。题目很简单，不同长度的钢条有不同的价格，假设切割不需要花钱，请问你有一段长为L的钢条，怎么切割能使获利最大。比如，切成w1,w2,w3...长度，对应单价为p1,p2,p3...，则总获利为w1p1+w2p2+w3p3...。

	如果暴力枚举所有的解肯定太慢，这个时候有这样一个思路：

	第一种情况：钢条不需要切割，第二种情况：钢条第一次在x处切割。

	所以， f(L)=max( p(L), f(x)+f(L-x) ) x在1到L-1之间遍历。当然，可以图省事只遍历一半，另一半是对称的。

	这就是动态规划的核心思想。

	然后是叫备忘机制。大家都知道斐波那契的递归写法：

	```C++
	int fib(int n) {
		if(n==1||n==2) return 1;
		else return fib(n-1)+fib(n-2);
	}
	```

	然而这么写速度会慢到爆表。但我们可以对中间值进行缓存：

	```C++
	int mem[10001]={-9999,1,1};
	int fib(int n) {
		if(mem[n]!=0) return mem[n];
		else {
			mem[n]=fib(n-1)+fib(n-2);
			return mem[n];
		}
	}
	```

	每一个值只需要计算一次。

	这就是所谓的备忘机制。

	最后谈一谈自底向上。刚才斐波那契的程序在n比较大的时候还是会爆表，这个时候可以换成递推的写法：

	```C++
	int mem[10001]={-9999,1,1};
	int fib(int n) {
		for(int i=3;i<=n;++i) {
			if(mem[i]!=0) continue;
			mem[i]=mem[i-1]+mem[i-2];
		}
		return mem[n];
	}
	```

	如果只求解一次的话，显然是递推的手法效率最高。

	比如像刚才那个钢条切割问题，递推的思路就是：

	首先，f(1)肯定等于p(1)啦，那f(2)是max(p(2),2*f(1))，f(3)是max(p(3),f(1)+f(2))，f(4)是max(p(4),f(1)+f(3),2*f(2))，依次类推。

	不过有一点，**动态规划要求最优子结构**。什么意思呢？先举个例子：有一个正方形，要求点之间的**最长简单路径**。顺时针排序1234，1到2、2到3最长简单路径当然是3，但是1到3的最长简单路径并不是6，而是2.

	也就是说，最优解应该是由另外若干个局部最优解组成的。满足这种性质的才可以用动态规划。

* 贪心

	书上用活动安排问题来引出的。

	假设有一个教室，很多人想在不同时间安排活动，现在为了让能进行活动的团体最多，应该用什么算法？

	首先，动态规划是肯定可以用的。然而，可以用更加简单的算法：贪心。

	也就是每次找最早结束的活动并安排之，然后处理剩下的活动。因为如果最优解能安排更多的活动，且不安排这个活动，随意找其他的活动，结束时间比这个活动晚，那么可以把那个活动替换成这个活动而不减少活动安排数，这样就矛盾了。

	上面的意思是，虽然最优解可能有很多个，但是每次找最早结束的肯定能得到其中一个最优解。

	当然，反过来也可以，每次找最晚开始的。对称嘛。

	贪心的适用范围比较窄，一般很难得到最优解，除非数学上证明。

* 摊还分析

	就是花式分析复杂度。有的时候直观不好判断，或者说是涉及到很多方面，这个时候可以用这种分析。

	第一种分析法：聚合分析。白话来讲：多次测量取平均值。书上用栈做例子，考虑多次push和multipop，但是你multipop的东西你肯定是之前push进去过的，所以虽然multipop的代价是O(n)，但是因为你之前是push进去的，所以m个push、multipop指令后的代价依然是O(m)而不是O(mn)。

	第二种分析法：核算法。白话来讲：信用与费用。信用是你规定部分操作会存储信用，而费用是各个操作会造成的代价。比如还是那个栈，假设push存2信用，那么push消耗1个，pop掉这个元素时也消耗1个，所以信用永远不会为负数，而m指令最多只需要2m个信用即可。

	第三种是势函数。相当于你当前的数据结构状态本身也可以进行计算，假设有个势。可以通过定义一个势来算各个操作的摊还代价。比如你定义栈的势为其元素个数，那么你pop的时候栈本身的势会减少，这部分可以视作支付了pop的代价；而push的时候，本身的操作代价加上势增长的代价为其摊还代价，最后还是变得和核算法一样了。(元素个数作势本身和信用相似)

* 动态表

	就是一个和std::vector很像的东西，增加以后扩充，减少到一定程度以后收缩。通过以上的摊还分析最终分析出了一个界(扩充肯定是满了扩到2倍，收缩的话要小于1/4才仅仅减半)来保证其性能，保证插入/删除的摊还代价为O(1).

	[回到索引](#menu)
	
---

---

<p id="other"></p>

### 5. 杂项

* Strassen

	用来加速矩阵乘法的。传统的分治递归需要做8遍乘法，但是通过奇怪的加减运算生成的矩阵后可以只做7次。具体没有太大的推导空间，在书上看看怎么算有个印象就可以了。复杂度也从O(n^(log8))变为O(n^log7)。

* 快速傅里叶变换

	用来加速多项式乘法的。首先是用点对来表示多项式，然后通过点对的乘法，然后再逆向推出结果的多项式。由于可以选取特殊的点对，使得计算点对和点对逆推复杂度比n平方要低。选取的点对就是e^(2pi*i/n)，即方程x^n=1的解。所以，在多项式中，x^m这个表达式可以不用O(m)就能计算的出来(有点快速幂的意味)，从而加快速度。

* 字符串匹配

	简单匹配：不说了，肯定都懂的。
	
	基于有限状态机：最快，但是状态机难以构造。和编译原理的DFA相似。
	
	KMP：每次匹配头部会保留局部信息，匹配失败有可能会跳很远再尝试匹配，可以加快速度。具体细节涉及到前缀与后缀，总体而言就是你当前匹配到一半的待匹配串后缀和前缀相同时，下次尝试匹配你可以直接跳到相同的那一部分去。很显然，如果一个串内所有字符都不相同，那么KMP会直接退化为简单匹配。
	
	当然，除了向后匹配，还有从后往前尝试匹配的，这么做速度通常会快些。(从前往后你有可能只能滑1个，但是从后往前一滑可能滑一整个窗口。)
	
	
* 图算法

	首先是遍历。深度优先与广度优先...基本上理解起来不会特别难吧。
	
	然后是最小生成树，Prim是找与外界相连的最短边并且新增所连的点；Krustal是每次找最短边并保证不回路。基础思路都是已有的子图与还未开发的对立的图找之间的最短的边。这里书内有些概念，还是要去自己看与理解的。然后是中国人原创的算法，破圈法，每次找到最长的边并且破坏掉并保证图的连通性；虽然复杂度高一些就是了。
	
	然后是单源最短路，有dijkstra和bellman-ford。dijkstra就是广度搜索的思路，每次找最短的并且扩充出去，可以这么理解：距离多长就代表中间有几个节点，广度优先搜索，看终点在第几个层次，不就是距离多远了嘛。但是缺点是不能应付非正值的边。bellman-ford则是所谓的松弛，当AB+BC<AC时就替换C到A的距离。对所有的边要做点个数次数的松弛，复杂度高但是可以应付负权边，并且能知道是否有负环。如果有负环，最后看是不是还有AB+BC<AC这种边存在。如果存在证明有负环。道理其实很简单，就是负环无论你松弛多少次都不够，而没负环的图松弛一定次数后就能保证最短路了。
	
	多源的话是Floyd-Warshall还有Johnson。Floyd-Warshall就是简单的三次方暴力循环，对每个点都对其他任意两点之间进行松弛。Johnson主要是应对稀疏图，首先跑单源的bellman-ford，对所有负边进行调整，调整到正值并且保证图的性质不变(当然不能简单的所有边同时加一个值，因为走的边数不同加的数字也不同了)；然后对所有点跑dijkstra，因为稀疏所以会比Floyd-Warshall快。跑完后还要把图变换回原来的图，不过最短路还是原来的那一条。
	
	各个算法有基于各个数据结构(斐波那契堆、二叉堆等)的实现与不同复杂度，还有各种适用条件。总体而言是思路不算难，但真要理解透彻也没那么简单。
	
	[回到索引](#menu)

---	

---

<p id="end"></p>

### 6. 总结

一个学期时间有限，不可能整本书都学完。以上内容基本上是所学，以及我所思。可能会有些小错误以及我理解错的地方，还请包涵。

《算法导论》本身就有其收藏价值(以及防身价值【笑】)。个人是不太理解为何这本书会写的这么不亲民。很多能简单描述思想的地方都是赤裸裸的数学和伪代码描述，令人脸扁。虽说不是否定数学和伪代码的必要性，但是若能兼顾两者，这本书想必会更精彩一些。